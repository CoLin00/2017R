---
title: "基于R的网络爬虫"
author: "李丁"
date: "2017年12月20日"
output:
  ioslides_presentation:
    highlight: pygments
    widescreen: yes
    css: ../lec.css
---

## 内容提纲

- 网络访问与网页基本结构
    - 网络访问
    - 网页结构
    
- 网络数据抓取的基本流程
    - 获取网页
    - 解析网页
    
- 用R进行网络数据抓取的示例

- 用python进行网络数据抓取

# 网络访问与网页基本结构

## 网络访问：请求过程{#myImagePage95}
![](pic/1.1.png) 

## 网络访问：请求过程{#myImagePage95}

建立连接：建立与URL(统一资源定位符,Uniform Resource Locators)或者IP地址如的连接.端口如80就像门。

发送请求：向目标服务器发送一串文本，采用特定的协议（格式）。常见网络通信协议有HTTP、TCP、UDP 等。

URL样式：scheme://hostname:port/path?querystring#fragment；

URL实例：http://nbachina.qq.com/a/20170914/004815.htm


HTTP全称是HyperText Transfer Protocal。即：超文本传输协议。HTTP是应用层协议，使用WEB浏览器浏览网页，浏览器和服务器之间就会通过HTTP协议进行数据的发送和接收。



## 网络访问：请求包 {#myImagePage95}

请求字符串（16进制码 ASCII码）

![](pic/1.2.jpg) 


## 网络访问：请求包结构 {#myImagePage95}

![](pic/1.4.jpg) 

请求包体的结构

![](pic/1.2.png) 

## 网络访问：响应包 {#myImagePage95}

响应包示例1

![](pic/1.6.jpg) 

响应包的结构

![](pic/1.5.png) 


## 网络访问:示例2 {#myImagePage95}

请求包（部分）：

GET /lidingruc/2017R HTTP/1.1 <br>
Host: github.com <br>
Connection: keep-alive <br>
Cache-Control: max-age=0 <br>
Upgrade-Insecure-Requests: 1 <br>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36

响应包（部分）：

HTTP/1.1 200 OK <br>
Server: GitHub.com <br>
Date: Tue, 19 Dec 2017 12:05:09 GMT <br>
Content-Type: text/html; charset=utf-8 <br>
Transfer-Encoding: chunked <br>
Status: 200 OK <br>
Cache-Control: no-cache <br>
Vary: X-PJAX



## 网络访问:请求类型 {#myImagePage95}

![](pic/1.8.png) 

## 网络访问:GET和POST 方法对比 {#myImagePage95}



GET方法中的参数是直接附带在URL中的，像这样： <br>
http://www.abc.com/?params1=val1&params2=val2

POST方法中的参数是经过编码后放在HTTP包的请求体中的。编码方式有“x-www-form-urlencoded”和“form-data”。 <br>
前者一般用来传输“key=value”键值对，如：user=123&pwd=456 <br>
form-data可以传输二进制。在我们需要传输一些如图片等文件时，就需要使用form-data。

GET用于信息获取，同一个URL请求只会返回同一个结果。<br>
POST表示可能会修改服务器上资源的请求。<br>
由于GET请求被放在了URL中，POST请求的内容并不在URL中。<br>
POST请求相对来说是不可见的，但不可见并不代表“安全”。 <br>
HTTP协议中数据包均采用明文传输，通过抓包都很容易被看到。一些浏览器对URL的长度做了限制，如需要传输较多的数据，建议使用POST请求。


## 网络访问:响应状态码 {#myImagePage95}

1xx：指示信息–表示请求已接收，继续处理 <br>
2xx：成功–表示请求已被成功接收、理解、接受 <br>
3xx：重定向–要完成请求必须进行更进一步的操作 <br>
4xx：客户端错误–请求有语法错误或请求无法实现 <br>
5xx：服务器端错误–服务器未能实现合法的请求 <br>

![](pic/1.9.png) 


## 网页结构 {#myImagePage60}

 - HTML   +     CSS   +  JavaScript
 - 结构    +     样式  +     功能
 - 房间设置  +   装修样式   +  电器



![](pic/2.png) 

## 网页结构:常用标签 {#myImagePage95}

![](pic/3.png) 

## 网页结构:常用标签 {#myImagePage95}

![](pic/4.png) 

## 网页结构:数结构 {#myImagePage0}

![](pic/5.png) 


## 网页结构：示例

HTML 基础- 4个实例 http://www.runoob.com/html/html-basic.html

HTML 教程 | 菜鸟教程  http://www.runoob.com/html/html-tutorial.html

## 网页结构：审查元素{#myImagePage95}

![](pic/1.10.png) 



# 网络数据抓取的基本流程

## 网络数据抓取：获取+解析

爬虫是一种利用代码模拟浏览器获取页面并根据HTML结构或正则表达式筛选获取有关信息的一种工具。在R里面我们通常用Rcurl包实现前一半的功能（模拟浏览器访问页面），用XML或者xml2包完成后一半功能（通过HTML树结构筛选提取信息）。

 - 获取网页

 - 解析网页
 
 - 提取信息
 
 
 
##  获取与解析网页的工具{#myImagePage95}

+ 请求库（将HTML或XML文件弄到本地）：

     - RCurl（旧,基于liburl爬虫C库的底层爬虫请求包）
     - httr（新,短小精悍，类似于python的Requests）

![](pic/6.png) 

##  获取与解析网页的工具


+ 解析库（在R对象中重建HTML或XML文件的层次结构）：

     - XML（以C语言为基础的libxml2库）
          htmlParse（DOM风格的解析器）
     - xml2(以C语言为基础的libxml2库)
          生成xml_node和xml_nodeset两个class
 
对于xml_nodeset，用[[]]来选出xml_node、或用[]选出xml_nodeset。[[只限单选，[可以多选。

对于xml_node，不能用[[或[；只能用xml_children()对xml_node或xml_nodeset取xml_nodeset。
 
 
##  获取与解析网页的工具 

+ 提取库(提取节点、文本、属性及其属性值)：
       1. XML +XPath
           -  getNodeSet()或xpathSApply()
      1. xml2+XPath
           - xml_value()或xml_find_all()
      1. selectr+CSS（XML和xml2解析结果）
          可用SelectorGadget定位节点信息
          

##  获取与解析网页的工具

哈德利的rvest封装了httr、xml2（解析库），同时默认加载了selectr、magrittr。主要用于解析。请求功能不足。可以进行管道操作%>%，同时支持css/XPtah表达式。复杂请求最好用原生的请求库，比如RCurl和httr。

- html_node(x, css, xpath)
- xml_node(x, css, xpath)



css和XPath在网页解析流程中各有优劣，相互结合、灵活运用，会给网络数据抓取的效率带来很大提升。

house_basic_inf<-web%>%html_nodes(".houseInfo")%>%html_text()



## 信息提取：XPath解析

1. “/”代表绝对路径。就是不可跳转的没有任何捷径的路径。

1. “//”代表相对路径。可随意跨越，不必严格按照节点层次和顺序遍历的路径。相对简洁的路径表达式。

1. “.”指代某路径本身。该符号专门用于对路径进行二次引用的需求，可理解为占位符，管道符号传参过程中处理左侧传入参数占位所用的特殊符号。

1. “*”指代任何内容。

1. “|”符号代表或条件，在正则中还是在函数逻辑符号中都是如此。

1. “@”：选择属性。

1. []符号。


Chrome插件SelectorGadget自动生成XPath表达式。

## 信息提取：CSS标签




## 信息提取：正则表达式

更为通用、更加底层的文本信息提取工具——正则表达式

使用一个字符串来描述、匹配一系列某个语法规则的字符串。通过特定的字母、数字以及特殊符号的灵活组合即可完成对任意字符串的匹配，从而达到提取相应文本信息的目的。

R默认的正则表达式风格包括基础文本处理函数和stringr包中的文本处理函数。

stringr包是Hadley Wickham 开发了一款专门进行文本处理的R包，它对基础的文本处理函数进行了扩展和整合，在一致性和易于理解性上都要优于基础函数。

示例：邮箱账户的正则表达式
[A-Za-z0-9\._+]+@[A-Za-z0-9]+\.(com|org|edu|net) 

正则表达式速查表
http://www.jb51.net/shouce/jquery1.82/regexp.html

在线测试正则表达式：
https://www.regexpal.com/

## 信息提取：正则表达式{#myImagePage95}

![](pic/8.png) 


## 信息提取：正则表达式{#myImagePage95}

![](pic/9.png) 
## 信息提取：正则表达式{#myImagePage95}

![](pic/10.png) 
##  最简单获取网页

西安交通大学校友捐赠榜：

http://alumni.xjtu.edu.cn:9090/donation/namelist?pageNo=1&pageSize=200&billnum=&donateUserName=&orderWay=&donationid=0


## 动态网页
AJAX（引发动作，实现网页修改）+XHR（实现异步通信，加载HTML/XML和JSON数据）

AJAX，全称叫做异步JavaScript和XML（Asynchronous JavaScript and XML）。不同的浏览器有自己的AJAX实现组件，有了AJAX技术之后，无需对整个网页进行刷新了，局部更新既不占用宽带又可以提高加载速度有没有。比如知乎首页，想看新内容？不断把网页下拉自动加载就好。

JavaScript嵌入HTML并对之进行修改的三种改进方法：
1. 以HTML中的 script标签为固定位置进行代码内嵌；
2. 对script元素中的src属性路径引用一个存放外部的JavaScript代码文件；
3. JavaScript代码直接出现在特定HTML元素属性里，也叫事件处理器。


## 动态网页：查看真实网页{#myImagePage95}
NETWORK + XHR
![](pic/7.png) 

##  动态网页抓取

 R工具 

    - RSelenium（推荐）
    - Rwebdriver(不很成熟)
    - seleniumpipes(结合RSelenium更高效)
    - rdom（高级封装，灵活性不够）
    - Rcrawler（支持多进程）
    - webshot（专门用于动态网页截图）

 要求

    1. 本地有selenium服务器并添加系统路径；
    2. 本地有plantomjs浏览器并添加系统路径；
    3. 安装了RSelenium包。



## 模拟登陆与复杂翻页



## 反爬虫

