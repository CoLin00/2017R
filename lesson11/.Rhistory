df = readHTMLTable(htmlParse(url),header=F)$tablelist
url = getURL('http://stuffgate.com/stuff/website/top-1000-sites')
df = readHTMLTable(htmlParse(url),header=F)$tablelist
url = getURL('http://stuffgate.com/stuff/website/top-1000-sites')
str(url)
url = getURL("http://stuffgate.com/stuff/website/top-1000-sites")
str(web)
web$doc
web$node
url <- 'https://en.wikipedia.org/wiki/List_of_most_popular_websites'
tables <-readHTMLTable(url,
stringsAsFactors =FALSE,
header = F)
url = 'https://en.wikipedia.org/wiki/List_of_most_popular_websites'
web = html(url,encoding = "UTF-8")
web = read_html(url,encoding = "UTF-8")
web$doc
url = getURL("https://en.wikipedia.org/wiki/List_of_most_popular_websites")
df = readHTMLTable(htmlParse(url),header=F)$tablelist
str(url)
nodes<-htmlParse(url)
nodes
str(nodes)
df = readHTMLTable(nodes,header=F)$tablelist
d = readHTMLTable(nodes,header=F)
d = readHTMLTable(nodes,header=F)[[1]]
View(d)
d = readHTMLTable(nodes,header=T)[[1]]
View(d)
?rvest
??rvest
library(rvest)
library(rvest)
url = 'https://en.wikipedia.org/wiki/List_of_most_popular_websites'
web = html(url,encoding = "UTF-8")
help("Deprecated")
library(xlm2)
url = 'https://en.wikipedia.org/wiki/List_of_most_popular_websites'
web = read_html(url,encoding = "UTF-8")
nodes=web %>% html_nodes("table")%>% .[[1]] %>% html_table(fill = TRUE)
View(nodes)
library(XML)
url <- 'https://en.wikipedia.org/wiki/List_of_most_popular_websites'
tables <-readHTMLTable(url,
stringsAsFactors =FALSE,
header = F)
data <- tables[[1]]
url <- 'https://en.wikipedia.org/wiki/List_of_most_popular_websites'
tables <-readHTMLTable(url,
stringsAsFactors =FALSE,
header = T)
u = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
tables = readHTMLTable(u)
names(tables)
url <- 'https://en.wikipedia.org/wiki/List_of_most_popular_websites'
doc = htmlParse(u)
library(httr)
url <- 'https://en.wikipedia.org/wiki/List_of_most_popular_websites'
web<-GET(url,.encoding = "utf-8")
doc = htmlParse(web)
tables <-readHTMLTable(doc,
stringsAsFactors =FALSE,
header = T)
data <- tables[[1]]
web<-read_html(url,.encoding = "utf-8")
doc = htmlParse(web)
tables <-readHTMLTable(doc,
stringsAsFactors =FALSE,
header = T)
data <- tables[[1]]
url <- 'https://stuffgate.com/stuff/website/top-1000-sites'
web<-GET(url,.encoding = "utf-8")
url <- 'http://stuffgate.com/stuff/website/top-1000-sites'
web<-GET(url,.encoding = "utf-8")
doc = htmlParse(web)
tables <-readHTMLTable(doc,
stringsAsFactors =FALSE,
header = T)
data <- tables[[1]]
data <- tables[[2]]
View(data)
rm(list=ls())
library(XML)
url <- 'http://stuffgate.com/stuff/website/top-1000-sites'
web<-read_html(url,.encoding = "utf-8")
doc = htmlParse(web)
tables <-readHTMLTable(doc,
stringsAsFactors =FALSE,
header = T)
data <- tables[[2]]
library(httr)
url <- 'http://stuffgate.com/stuff/website/top-1000-sites'
web<-GET(url,.encoding = "utf-8")
doc = htmlParse(web)
tables <-readHTMLTable(doc,
stringsAsFactors =FALSE,
header = T)
data <- tables[[2]]
library(httr)  ## 获取网页的另一张形式
web<-GET(url,.encoding = "utf-8")
doc<-content(web,as="parsed")
tables <-readHTMLTable(doc,
stringsAsFactors =FALSE,
header = T)
data <- tables[[2]]
library(httr)  ## 获取网页的另一张形式
web<-GET(url,.encoding = "utf-8")
doc<-content(web)
tables <-readHTMLTable(doc,
stringsAsFactors =FALSE,
header = T)
data <- tables[[2]]
str(doc)
require(RCurl) #调用curl
name<-"leadingsoci@163.com"
pwd<-"521chunxia_=ruc"
host=c(
"User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.2.12) Gecko/20101026 Firefox/3.6.12",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"="zh-cn,zh;q=0.5",
"Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7",
"Keep-Alive"="115",
"Connection"="keep-alive",
"Content-Type"="application/x-www-form-urlencoded; charset=UTF-8",
"Pragma"="no-cache",
"Cache-Control"="no-cache")
cookie_file='/Users/liding/E/Bdata/rtemp/cookie.txt'
myHttpheader<- c(
"Host"= "dj.renren.com",
"User-Agent" ="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:52.0) Gecko/20100101 Firefox/52.0",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"= "zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3",
"Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7",
"Referer"="http://www.renren.com/SysHome.do",
"Connection"= "keep-alive"
)
d =debugGatherer()
temp<- getURL("http://www.renren.com/PLogin.do",header=host,
debugfunction=d$update,verbose= TRUE)
cat(d$value()[3])
cat(d$value()[2])
d <- debugGatherer()
cH <- getCurlHandle(followlocation=T, verbose=T,
debugfunction=d$update,
ssl.verifyhost=F, ssl.verifypeer=F,
cookiejar='./cookies', cookiefile='./cookies')
pinfo <- c(
'email'=name,
'password'=pwd
)
x <- try(ttt <- postForm('http://www.renren.com/PLogin.do', header=host,
.params=pinfo,  style='post', curl=cH,
.encoding = 'utf-8'))
host=c(
"User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.2.12) Gecko/20101026 Firefox/3.6.12",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"="zh-cn,zh;q=0.5",
"Accept-Charset"="utf-8;q=0.7,*;q=0.7",
"Keep-Alive"="115",
"Connection"="keep-alive",
"Content-Type"="application/x-www-form-urlencoded; charset=UTF-8",
"Pragma"="no-cache",
"Cache-Control"="no-cache")
# 登录
#url.exists("http://www.renren.com/PLogin.do")
x <- try(ttt <- postForm('http://www.renren.com/PLogin.do', header=host,
.params=pinfo,  style='post', curl=cH,
.encoding = 'utf-8'))
cH <- getCurlHandle(followlocation=T, verbose=T,
debugfunction=d$update,
ssl.verifyhost=F, ssl.verifypeer=F,
cookiejar='./cookies') #, cookiefile=cookie_file
x <- try(ttt <- postForm('http://www.renren.com/PLogin.do', header=host,
.params=pinfo,  style='post', curl=cH,
.encoding = 'utf-8'))
library(httr)
temp<- POST("http://www.renren.com/PLogin.do",header=host,
config=pinfo,verbose= TRUE)
temp<- POST("http://www.renren.com/PLogin.do",add_headers(.headers=host),
authenticate("leadingsoci@163.com","521chunxia_=ruc"),verbose= TRUE)
temp<- POST("http://www.renren.com/PLogin.do",add_headers(.headers=host),
authenticate("leadingsoci@163.com","521chunxia_=ruc",type="digest"),verbose= TRUE)
library(RCurl)
curl <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt", curl=curl)
postForm("http://www.renren.com/PLogin.do", email="leadingsoci@163.com", passwd="521chunxia_=ruc", curl=curl)
getURL("http://www.renren.com/SysHome.do", curl=curl)
x <- try(ttt <- postForm("http://www.renren.com/PLogin.do", email="leadingsoci@163.com", passwd="521chunxia_=ruc", curl=curl))
library(RCurl)
curl <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt", curl=curl)
x <- try(ttt <- postForm("http://3g.renren.com/login.do", email="leadingsoci@163.com", passwd="521chunxia_=ruc", curl=curl))
getURL("http://3g.renren.com/Home.do", curl=curl)
str(x)
hostm=c(
"Accept" ="text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
"Accept-Language"="zh-CN,zh;q=0.8,en;q=0.6",
"Cache-Control"= "max-age=0",
"Connection"="keep-alive",
"Content-Type" ="application/x-www-form-urlencoded",
"Host"="3g.renren.com",
"Origin"="http://3g.renren.com",
"Referer"="http://3g.renren.com/login.do?",
"Upgrade-Insecure-Requests"=1,
"User-Agent"="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36"
)
d =debugGatherer()
temp<- getURL("http://3g.renren.com/Login.do",header=hostm,
debugfunction=d$update,verbose= TRUE)
cat(d$value()[3])
cat(d$value()[2])
d <- debugGatherer()
cH <- getCurlHandle(followlocation=T, verbose=T,
debugfunction=d$update,
ssl.verifyhost=F, ssl.verifypeer=F,
cookiejar='./cookies', cookiefile='./cookies')
pinfo <- c(
'email'=name,
'password'=pwd
)
lbskey<-"1489886897574WzGjs6cPjr5cPSvCfqdA"
pinfo <- c(
'email'=name,
'password'=pwd,
'lbskey'=lbskey
)
x <- try(ttt <- postForm('http://3g.renren.com/Login.do', header=hostm,
.params=pinfo,  style='post', curl=cH,
.encoding = 'utf-8'))
x <- try(ttt <- postForm('http://3g.renren.com/Login.do?', header=hostm,
.params=pinfo,  style='post', curl=cH,
.encoding = 'utf-8'))
x <- try(ttt <- postForm('http://3g.renren.com/login.do?autoLogin=true&', header=hostm,
.params=pinfo,  style='post', curl=cH,
.encoding = 'utf-8'))
str(x)
cat(x,file="aaa.htlm")
getwd()
cat(x,file="aaa.html")
d =debugGatherer()
temp<- getURL("http://3g.renren.com/Login.do?",header=hostm,
debugfunction=d$update,verbose= TRUE)
cat(d$value()[3])
cat(d$value()[2])
d =debugGatherer()
temp<- getURL("http://3g.renren.com/Login.do?autoLogin=true&",header=hostm,
debugfunction=d$update,verbose= TRUE)
cat(d$value()[3])
cat(d$value()[2])
d <- debugGatherer()
cH <- getCurlHandle(followlocation=T, verbose=T,
debugfunction=d$update,
ssl.verifyhost=F, ssl.verifypeer=F,
cookiejar='./cookies', cookiefile='./cookies')
pinfo <- c(
'email'=name,
'password'=pwd,
'lbskey'=lbskey
)
d <- debugGatherer()
cH <- getCurlHandle(followlocation=T, verbose=T,
debugfunction=d$update,
ssl.verifyhost=F, ssl.verifypeer=F,
cookiejar='./cookies') #, cookiefile=cookie_file
#url.exists("http://www.renren.com/PLogin.do")
x <- try(ttt <- postForm('http://3g.renren.com/login.do?autoLogin=true&', header=hostm,
.params=pinfo,  style='post', curl=cH,
.encoding = 'utf-8'))
cat(x,file="ab.htlm")
cat(x,file="ab.html")
curl <- getCurlHandle() #虚拟一个浏览器
curlSetOpt(cookiejar=tempfile(), curl=curl) #生成cookie
getURL("http://www.renren.com/SysHome.do", curl=curl) #填充cookie
web0<-getURL("http://www.renren.com/SysHome.do", curl=curl) #填充cookie
cat(web0,file="abb.html")
the0url <- 'http://3g.renren.com/friendlist.do?&sid=qCsb7h-weIDuaqHmhIuYIs&mt40zx&htf=3'
the0get <- getURL(the0url, curl=cH, .encoding='UTF-8')
# write(the0get, 'xxx.txt')
uid <- gsub('^.*\n\'id\':\'(\\d+)\'.*$', '\\1', the0get)
cat(the0get,file="ab.html")
url.exists("http://www.renren.com/home")
library(RCurl)
library(XML)
url <-"http://bj.xiaozhu.com/fangzi/5098280314.html"
doc <- htmlTreeParse(url, useInternalNodes=T,encoding='utf-8')
#原因为htmlParse可以抓取http的页面，不能抓取https的页面
nodi<-getNodeSet(doc, "//h4")
getNodeSet(doc, "//h4/em")
# 提取内容
xmlValue(nodi[[1]])
# 提取属性
xmlValue(getNodeSet(doc, "//title")[[1]])
getNodeSet(doc, "//head")
getNodeSet(doc, "//h6")
getNodeSet(doc, "//h6")[[1]]
summary(getNodeSet(doc, "//h6"))
getNodeSet(doc, c("//h6","//h4"))
getNodeSet(doc, "//a[@class='lorder_name']")[[1]]
Encoding(getNodeSet(doc, "//h6")[[1]]) = "UTF-8"
biaoti<-getNodeSet(doc, "//h6")[[1]]
biaoti
Encoding(biaoti) = "UTF-8"
biaoti<-as.vecter(biaoti)
biaoti<-as.vector(biaoti)
biaoti<-as.vector(biaoti)
biaoti<-as.character(biaoti)
biaoti<-getNodeSet(doc, "//h6")[[1]]
biaoti
biaoti<-xmlValue(getNodeSet(doc, "//h6")[[1]])
Encoding(biaoti) = "UTF-8"
biaoti
big  <- replicate(100, factor(sample(c('AA','AB','BB', NA), 1e6, T)), simplify = F)
bigDT <- as.data.table(big)
system.time({
for(name in names(big)){
setattr(big[[name]],'levels',1:-1)
}
})
str(big)
big  <- replicate(100, factor(sample(c('AA','AB','BB', NA), 1e6, T)), simplify = F)
bigDT <- as.data.table(big)
library(data.table)
bigDT <- as.data.table(big)
View(bigDT)
system.time({
for(name in names(bigDT)){
setattr(bigDT[[name]],'levels',1:-1)
}
})
table(bigDT$V1)
system.time({
for(name in names(big)){
setattr(big[[name]],'levels',1:-1)
}
})
table(big$v1)
str(bigDT)
system.time({
for(name in names(big)){
setattr(big[[name]],'levels',1:-1)
}
})
bigDT <- as.data.table(big)
str(bigDT)
table(bigDT$V1)
libray(sna)
install.packages(sna)
install.packages("sna")
install.packages("statnet")
library(statnet)
# 首先随机生成3个由10个节点构成的有向网络
g=array(dim=c(3,10,10))
g[1,,] = rgraph(10)
g[2,,] = rgraph(10,tprob=g[1,,]*0.8) # 设置g1和g2两个网络强相关
g[3,,] = 1; g[3,1,2] = 0 # g3接近于一个派系（clique）
# 绘制这3个网络
par(mfrow=c(1,3))
for(i in 1:3) {
gplot(g[i,,],usecurv=TRUE, mode = "fruchtermanreingold",
vertex.sides=3:8)}
#计算网络的相关矩阵
gcor(g)
j = rgraph(5) # 随机生成一个网络
j  #看一下这个网络的矩阵形式
rmperm(j) #随机置换后的网络的矩阵形式
q.12 = qaptest(g, gcor, g1 = 1, g2 = 2)
q.13 = qaptest(g, gcor, g1 = 1, g2 = 3)
# 看一下QAP输出的结果
par(mfrow=c(1,2))
summary(q.12)
plot(q.12)
summary(q.13)
plot(q.13)
?bygroup
?groupby
??goupby
??bygroup
library(XML)
url <-"http://bj.xiaozhu.com/fangzi/5098280314.html"
doc <- htmlTreeParse(url, useInternalNodes=T,encoding='utf-8')
nodi<-getNodeSet(doc, "//h4")
getNodeSet(doc, "//h4/em")
xpathApply(doc,"//h4/em")
xpathApply(doc,"//h4/em/text()")
sapply(getNodeSet(doc, "//h4/em") ,xmlValue)
getNodeSet(doc,"//title")
sapply(getNodeSet(doc, "//title") ,xmlName)
sapply(getNodeSet(doc, "//title") ,xmlValue)
doc["//title"]
doc["//head"]
getNodeSet(doc,"//head")
summary(getNodeSet(doc, "//h6"))
length(getNodeSet(doc, "//h6/text()"))
doc["//title/text()"]
doc["//title"]
print(doc["//title/text()"])
doc["//title"]
doc["//title/text()"]
doc["//head/description"]
doc["//head@description"]
doc["//head/*"]
doc["//head"]
doc["//head/*"]
getNodeSet(doc,"//head")
summary(getNodeSet(doc, "//head"))
doc["//head/*"]
doc["//head/title"]
doc["//head/link"]
doc["//head/meta"]
doc["//head/meta"]
doc["//head/*"]
doc["//head/link"]
getNodeSet(doc,"//head")
nodi<-getNodeSet(doc, "//h4")
getNodeSet(doc, "//h4/em")
xpathApply(doc,"//h4/em")
xpathApply(doc,"//h6")
nodh4<-getNodeSet(doc, "//h4")
getNodeSet(doc, "//h4/em")
nodh4
doc["//h4[@class="zhima_score"]"]
doc["//h4[@class='zhima_score']"]
doc["//h4[@class='zhima_score']/text()"]
doc["//head[1]"]
doc["//head[[1]]"]
doc["//head"][1]
doc["//head"][[1]]
doc["//head"][1][[1]]
doc["//head"][[1]]
doc["//head"]
doc["//head$meta"]
doc["//head/meta[@name='keywords']"]
doc["//head/link[@rel='stylesheet']"]
xpathSApply(doc,"//h6")
xpathApply(doc,"//h4/em/text()")
sapply(getNodeSet(doc, "//h4/em") ,xmlValue)
getNodeSet(doc,"//title")
sapply(getNodeSet(doc, "//title") ,xmlName)
sapply(getNodeSet(doc, "//title") ,xmlValue)
library(XML)
url <-"http://bj.xiaozhu.com/fangzi/5098280314.html"
#doc <- htmlTreeParse(url,encoding='utf-8')  #注意返回的结果是不一样的
doc <- htmlTreeParse(url, useInternalNodes=T,encoding='utf-8')
doc["//title"]
doc["//title/text()"]
doc["//head"]
doc["//head/*"]
doc["//head/meta"]
doc["//head/title"]
ti<-doc["//head/title"]
ti
getNodeSet(doc,"//head")
nodh4<-getNodeSet(doc, "//h4")
getNodeSet(doc, "//h4/em")
xpathApply(doc,"//h4/em/text()")
sapply(getNodeSet(doc, "//h4/em") ,xmlValue)
getNodeSet(doc, "//h6")
summary(getNodeSet(doc, "//h6"))
length(getNodeSet(doc, "//h6/text()"))
h6<-getNodeSet(doc, "//h6")
h6
setwd("/Users/liding/E/Bdata/rtemp/sna/Polnet2016/")
# If you don't know the path to the folder and you're in RStudio, go to the
# "Session" menu -> "Set Working Directory" -> "To Source File Location"
# --DATASET 1: edgelist--
nodes <- read.csv("./Data/Dataset1-Media-Example-NODES.csv", header=T, as.is=T)
links <- read.csv("./Data/Dataset1-Media-Example-EDGES.csv", header=T, as.is=T)
# Examine the data:
head(nodes)
head(links)
nrow(nodes); length(unique(nodes$id))
nrow(links); nrow(unique(links[,c("from", "to")]))
nrow(unique(links[,c("from", "to", "type")]))
links <- aggregate(links[,3], links[,-3], sum)
links <- links[order(links$from, links$to),]
links <- read.csv("./Data/Dataset1-Media-Example-EDGES.csv", header=T, as.is=T)
links[,3]
links[,-3]
links <- aggregate(links[,3], links[,-3], sum)
links <- links[order(links$from, links$to),]
colnames(links)[4] <- "weight"
rownames(links) <- NULL
nrow(links); nrow(unique(links[,c("from", "to")]))
nodes2 <- read.csv("./Data/Dataset2-Media-User-Example-NODES.csv", header=T, as.is=T)
links2 <- read.csv("./Data/Dataset2-Media-User-Example-EDGES.csv", header=T, row.names=1)
head(nodes2)
head(links2)
links2 <- as.matrix(links2)
dim(links2)
dim(nodes2)
View(links)
library("igraph")
net <- graph_from_data_frame(d=links, vertices=nodes, directed=T)
class(net)
net
E(net)
V(net)
E(net)$type
V(net)$media
V(net)[media=="BBC"]
E(net)[type=="mention"]
as_edgelist(net, names=T)
as_adjacency_matrix(net, attr="weight")
as_data_frame(net, what="edges")
as_data_frame(net, what="vertices")
net[1,]
net[5,7]
plot(net) # not pretty!
plot(net) # not pretty!
plot(net) # not pretty!
net <- simplify(net, remove.multiple = F, remove.loops = T)
plot(net, edge.arrow.size=.4,vertex.label=NA)
